<!DOCTYPE html>
<html style="display: none;" lang="zh">
    <head>
    <meta charset="utf-8">
    <!--
        © Material Theme
        https://github.com/bollnh/hexo-theme-material
        Version: 1.5.6 -->
    <script>
        window.materialVersion = "1.5.6"
        // Delete localstorage with these tags
        window.oldVersion = [
            'codestartv1',
            '1.3.4',
            '1.4.0',
            '1.4.0b1',
            '1.5.0',
            '1.5.2',
            '1.5.5'
        ]
    </script>

    <!-- dns prefetch -->
    <meta http-equiv="x-dns-prefetch-control" content="on">





    <link rel="dns-prefetch" href="https://.disqus.com"/>





    <link rel="dns-prefetch" href="https://fonts.googleapis.com"/>





    <!-- Meta & Info -->
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="renderer" content="webkit">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!-- Title -->
    
    <title>
        
            RLTransformer | 
        
        Hexo
    </title>

    <!-- Favicons -->
    <link rel="icon shortcut" type="image/ico" href="/img/favicon.svg">
    <link rel="icon" href="/img/favicon.svg">

    <meta name="format-detection" content="telephone=no"/>
    <meta name="description" itemprop="description" content="">
    <meta name="keywords" content=",Reinforcement Learning">
    <meta name="theme-color" content="#0097A7">

    <!-- Disable Fucking Bloody Baidu Tranformation -->
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />

    <!--[if lte IE 9]>
        <link rel="stylesheet" href="/css/ie-blocker.css">

        
            <script src="/js/ie-blocker.zhCN.js"></script>
        
    <![endif]-->

    <!-- Import lsloader -->
    <script>(function(){window.lsloader={jsRunSequence:[],jsnamemap:{},cssnamemap:{}};lsloader.removeLS=function(a){try{localStorage.removeItem(a)}catch(b){}};lsloader.setLS=function(a,c){try{localStorage.setItem(a,c)}catch(b){}};lsloader.getLS=function(a){var c="";try{c=localStorage.getItem(a)}catch(b){c=""}return c};versionString="/*"+(window.materialVersion||"unknownVersion")+"*/";lsloader.clean=function(){try{var b=[];for(var a=0;a<localStorage.length;a++){b.push(localStorage.key(a))}b.forEach(function(e){var f=lsloader.getLS(e);if(window.oldVersion){var d=window.oldVersion.reduce(function(g,h){return g||f.indexOf("/*"+h+"*/")!==-1},false);if(d){lsloader.removeLS(e)}}})}catch(c){}};lsloader.clean();lsloader.load=function(f,a,b,d){if(typeof b==="boolean"){d=b;b=undefined}d=d||false;b=b||function(){};var e;e=this.getLS(f);if(e&&e.indexOf(versionString)===-1){this.removeLS(f);this.requestResource(f,a,b,d);return}if(e){var c=e.split(versionString)[0];if(c!=a){console.log("reload:"+a);this.removeLS(f);this.requestResource(f,a,b,d);return}e=e.split(versionString)[1];if(d){this.jsRunSequence.push({name:f,code:e});this.runjs(a,f,e)}else{document.getElementById(f).appendChild(document.createTextNode(e));b()}}else{this.requestResource(f,a,b,d)}};lsloader.requestResource=function(b,e,a,c){var d=this;if(c){this.iojs(e,b,function(h,f,g){d.setLS(f,h+versionString+g);d.runjs(h,f,g)})}else{this.iocss(e,b,function(f){document.getElementById(b).appendChild(document.createTextNode(f));d.setLS(b,e+versionString+f)},a)}};lsloader.iojs=function(d,b,g){var a=this;a.jsRunSequence.push({name:b,code:""});try{var f=new XMLHttpRequest();f.open("get",d,true);f.onreadystatechange=function(){if(f.readyState==4){if((f.status>=200&&f.status<300)||f.status==304){if(f.response!=""){g(d,b,f.response);return}}a.jsfallback(d,b)}};f.send(null)}catch(c){a.jsfallback(d,b)}};lsloader.iocss=function(f,c,h,a){var b=this;try{var g=new XMLHttpRequest();g.open("get",f,true);g.onreadystatechange=function(){if(g.readyState==4){if((g.status>=200&&g.status<300)||g.status==304){if(g.response!=""){h(g.response);a();return}}b.cssfallback(f,c,a)}};g.send(null)}catch(d){b.cssfallback(f,c,a)}};lsloader.iofonts=function(f,c,h,a){var b=this;try{var g=new XMLHttpRequest();g.open("get",f,true);g.onreadystatechange=function(){if(g.readyState==4){if((g.status>=200&&g.status<300)||g.status==304){if(g.response!=""){h(g.response);a();return}}b.cssfallback(f,c,a)}};g.send(null)}catch(d){b.cssfallback(f,c,a)}};lsloader.runjs=function(f,c,e){if(!!c&&!!e){for(var b in this.jsRunSequence){if(this.jsRunSequence[b].name==c){this.jsRunSequence[b].code=e}}}if(!!this.jsRunSequence[0]&&!!this.jsRunSequence[0].code&&this.jsRunSequence[0].status!="failed"){var a=document.createElement("script");a.appendChild(document.createTextNode(this.jsRunSequence[0].code));a.type="text/javascript";document.getElementsByTagName("head")[0].appendChild(a);this.jsRunSequence.shift();if(this.jsRunSequence.length>0){this.runjs()}}else{if(!!this.jsRunSequence[0]&&this.jsRunSequence[0].status=="failed"){var d=this;var a=document.createElement("script");a.src=this.jsRunSequence[0].path;a.type="text/javascript";this.jsRunSequence[0].status="loading";a.onload=function(){d.jsRunSequence.shift();if(d.jsRunSequence.length>0){d.runjs()}};document.body.appendChild(a)}}};lsloader.tagLoad=function(b,a){this.jsRunSequence.push({name:a,code:"",path:b,status:"failed"});this.runjs()};lsloader.jsfallback=function(c,b){if(!!this.jsnamemap[b]){return}else{this.jsnamemap[b]=b}for(var a in this.jsRunSequence){if(this.jsRunSequence[a].name==b){this.jsRunSequence[a].code="";this.jsRunSequence[a].status="failed";this.jsRunSequence[a].path=c}}this.runjs()};lsloader.cssfallback=function(e,c,b){if(!!this.cssnamemap[c]){return}else{this.cssnamemap[c]=1}var d=document.createElement("link");d.type="text/css";d.href=e;d.rel="stylesheet";d.onload=d.onerror=b;var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(d,a)};lsloader.runInlineScript=function(c,b){var a=document.getElementById(b).innerText;this.jsRunSequence.push({name:c,code:a});this.runjs()}})();</script>

    <!-- Import queue -->
    <script>function Queue(){this.dataStore=[];this.offer=b;this.poll=d;this.execNext=a;this.debug=false;this.startDebug=c;function b(e){if(this.debug){console.log("Offered a Queued Function.")}if(typeof e==="function"){this.dataStore.push(e)}else{console.log("You must offer a function.")}}function d(){if(this.debug){console.log("Polled a Queued Function.")}return this.dataStore.shift()}function a(){var e=this.poll();if(e!==undefined){if(this.debug){console.log("Run a Queued Function.")}e()}}function c(){this.debug=true}}var queue=new Queue();</script>

    <!-- Import CSS -->
    
        <style id="material_css"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("material_css","/css/material.min.css?Z7a72R1E4SxzBKR/WGctOA==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>
        <style id="style_css"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("style_css","/css/style.min.css?NKhlKQkXw/c66TR5p4wO+w==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>

        

    

    

    <!-- Config CSS -->

<!-- Other Styles -->
<style>
  body, html {
    font-family: Roboto, "Helvetica Neue", Helvetica, "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "微软雅黑", Arial, sans-serif;
    overflow-x: hidden !important;
  }
  
  code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  a {
    color: #00838F;
  }

  .mdl-card__media,
  #search-label,
  #search-form-label:after,
  #scheme-Paradox .hot_tags-count,
  #scheme-Paradox .sidebar_archives-count,
  #scheme-Paradox .sidebar-colored .sidebar-header,
  #scheme-Paradox .sidebar-colored .sidebar-badge{
    background-color: #0097A7 !important;
  }

  /* Sidebar User Drop Down Menu Text Color */
  #scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:hover,
  #scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:focus {
    color: #0097A7 !important;
  }

  #post_entry-right-info,
  .sidebar-colored .sidebar-nav li:hover > a,
  .sidebar-colored .sidebar-nav li:hover > a i,
  .sidebar-colored .sidebar-nav li > a:hover,
  .sidebar-colored .sidebar-nav li > a:hover i,
  .sidebar-colored .sidebar-nav li > a:focus i,
  .sidebar-colored .sidebar-nav > .open > a,
  .sidebar-colored .sidebar-nav > .open > a:hover,
  .sidebar-colored .sidebar-nav > .open > a:focus,
  #ds-reset #ds-ctx .ds-ctx-entry .ds-ctx-head a {
    color: #0097A7 !important;
  }

  .toTop {
    background: #757575 !important;
  }

  .material-layout .material-post>.material-nav,
  .material-layout .material-index>.material-nav,
  .material-nav a {
    color: #757575;
  }

  #scheme-Paradox .MD-burger-layer {
    background-color: #757575;
  }

  #scheme-Paradox #post-toc-trigger-btn {
    color: #757575;
  }

  .post-toc a:hover {
    color: #00838F;
    text-decoration: underline;
  }

</style>


<!-- Theme Background Related-->

    <style>
      body{
        background-color: #F5F5F5;
      }

      /* blog_info bottom background */
      #scheme-Paradox .material-layout .something-else .mdl-card__supporting-text{
        background-color: #fff;
      }
    </style>




<!-- Fade Effect -->

    <style>
      .fade {
        transition: all 800ms linear;
        -webkit-transform: translate3d(0,0,0);
        -moz-transform: translate3d(0,0,0);
        -ms-transform: translate3d(0,0,0);
        -o-transform: translate3d(0,0,0);
        transform: translate3d(0,0,0);
        opacity: 1;
      }

      .fade.out{
        opacity: 0;
      }
    </style>


<!-- Import Font -->
<!-- Import Roboto -->

    <link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet">


<!-- Import Material Icons -->


    <style id="material_icons"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("material_icons","/css/material-icons.css?pqhB/Rd/ab0H2+kZp0RDmw==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>




    <!-- Import jQuery -->
    
        <script>lsloader.load("jq_js","/js/jquery.min.js?qcusAULNeBksqffqUM2+Ig==", true)</script>
    

    <!-- WebAPP Icons -->
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="application-name" content="Hexo">
    <meta name="msapplication-starturl" content="http://gaoustcer.github.io/2023/04/04/RLTransformer/">
    <meta name="msapplication-navbutton-color" content="#0097A7">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-title" content="Hexo">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon" href="/img/favicon.svg">

    <!-- Site Verification -->
    
    

    <!-- RSS -->
    

    <!-- The Open Graph protocol -->
    <meta property="og:url" content="http://gaoustcer.github.io/2023/04/04/RLTransformer/">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="RLTransformer | Hexo">
    <meta property="og:image" content="/img/favicon.svg">
    <meta property="og:description" content="">
    <meta property="og:article:tag" content="Reinforcement Learning"> 

    
        <meta property="article:published_time" content="Tue Apr 04 2023 22:24:14 GMT+0800">
        <meta property="article:modified_time" content="Wed Apr 05 2023 00:35:06 GMT+0800">
    

    <!-- The Twitter Card protocol -->
    <meta name="twitter:card" content="summary_large_image">

    <!-- Add canonical link for SEO -->
    
        <link rel="canonical" href="http://gaoustcer.github.io/2023/04/04/RLTransformer/index.html" />
    

    <!-- Structured-data for SEO -->
    
        


<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "mainEntityOfPage": "http://gaoustcer.github.io/2023/04/04/RLTransformer/index.html",
    "headline": "RLTransformer",
    "datePublished": "Tue Apr 04 2023 22:24:14 GMT+0800",
    "dateModified": "Wed Apr 05 2023 00:35:06 GMT+0800",
    "author": {
        "@type": "Person",
        "name": "Gaoustcer",
        "image": {
            "@type": "ImageObject",
            "url": "/img/avatar.png"
        },
        "description": "Hi, nice to meet you"
    },
    "publisher": {
        "@type": "Organization",
        "name": "Hexo",
        "logo": {
            "@type":"ImageObject",
            "url": "/img/favicon.svg"
        }
    },
    "keywords": ",Reinforcement Learning",
    "description": "",
}
</script>


    

    <!-- Analytics -->
    
    
    

    <!-- Custom Head -->
    

<meta name="generator" content="Hexo 6.3.0"></head>


    
        <body id="scheme-Paradox" class="lazy">
            <div class="material-layout  mdl-js-layout has-drawer is-upgraded">
                

                <!-- Main Container -->
                <main class="material-layout__content" id="main">

                    <!-- Top Anchor -->
                    <div id="top"></div>

                    
                        <!-- Hamburger Button -->
                        <button class="MD-burger-icon sidebar-toggle">
                            <span id="MD-burger-id" class="MD-burger-layer"></span>
                        </button>
                    

                    <!-- Post TOC -->

    
    <!-- Back Button -->
    <!--
    <div class="material-back" id="backhome-div" tabindex="0">
        <a class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon"
           href="#" onclick="window.history.back();return false;"
           target="_self"
           role="button"
           data-upgraded=",MaterialButton,MaterialRipple">
            <i class="material-icons" role="presentation">arrow_back</i>
            <span class="mdl-button__ripple-container">
                <span class="mdl-ripple"></span>
            </span>
        </a>
    </div>
    -->


    <!-- Left aligned menu below button -->
    
    
    <button id="post-toc-trigger-btn"
        class="mdl-button mdl-js-button mdl-button--icon">
        <i class="material-icons">format_list_numbered</i>
    </button>

    <ul class="post-toc-wrap mdl-menu mdl-menu--bottom-left mdl-js-menu mdl-js-ripple-effect" for="post-toc-trigger-btn" style="max-height:80vh; overflow-y:scroll;">
        <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#RL-Transformer"><span class="post-toc-number">1.</span> <span class="post-toc-text">RL Transformer</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E5%9B%9E%E9%A1%BE%EF%BC%9ATransformer"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">回顾：Transformer</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%9B%9E%E9%A1%BEAttention%E6%9C%BA%E5%88%B6"><span class="post-toc-number">1.1.1.</span> <span class="post-toc-text">回顾Attention机制</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Attention-for-Seq2Seq"><span class="post-toc-number">1.1.2.</span> <span class="post-toc-text">Attention for Seq2Seq</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#From-Attention-to-Self-Attention"><span class="post-toc-number">1.1.3.</span> <span class="post-toc-text">From Attention to Self-Attention</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#From-Attention-to-Multi-head-Attention"><span class="post-toc-number">1.1.4.</span> <span class="post-toc-text">From Attention to Multi-head Attention</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#From-Single-head-Self-Attention-to-Encoder"><span class="post-toc-number">1.1.5.</span> <span class="post-toc-text">From Single-head Self-Attention to Encoder</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Decoder%E7%BD%91%E7%BB%9C"><span class="post-toc-number">1.1.6.</span> <span class="post-toc-text">Decoder网络</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Pytorch%E4%B8%ADTransformer%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">Pytorch中Transformer的实现</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Embedding"><span class="post-toc-number">1.2.1.</span> <span class="post-toc-text">Embedding</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Position-Embedding"><span class="post-toc-number">1.2.2.</span> <span class="post-toc-text">Position Embedding</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Transformer-from-pytorch%E2%80%94%E2%80%94target-Masking"><span class="post-toc-number">1.2.3.</span> <span class="post-toc-text">Transformer from pytorch——target Masking</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Padding-Masking"><span class="post-toc-number">1.2.4.</span> <span class="post-toc-text">Padding Masking</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Offline-Reinforcement-Learning-as-One-Big-Sequence-Modeling-Problem"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">Offline Reinforcement Learning as One Big Sequence Modeling Problem</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary"><span class="post-toc-number">1.3.1.</span> <span class="post-toc-text">Summary</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E8%BD%A8%E8%BF%B9"><span class="post-toc-number">1.3.2.</span> <span class="post-toc-text">轨迹</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E6%9E%84%E5%BB%BA%E5%AD%97%E5%85%B8"><span class="post-toc-number">1.3.3.</span> <span class="post-toc-text">构建字典</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%8E%9F%E6%96%87%EF%BC%88Reinforcement-Learning-and-Control-as-Sequence-Modeling%EF%BC%89"><span class="post-toc-number">1.3.4.</span> <span class="post-toc-text">原文（Reinforcement Learning and Control as Sequence Modeling）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E7%A6%BB%E6%95%A3%E5%8C%96-%E7%94%A8%E4%BA%8E%E8%BE%93%E5%87%BA%E4%BC%BC%E7%84%B6%E6%A6%82%E7%8E%87"><span class="post-toc-number">1.3.4.1.</span> <span class="post-toc-text">连续空间离散化(用于输出似然概率)</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="post-toc-number">1.3.4.2.</span> <span class="post-toc-text">损失函数</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E7%AE%97%E6%B3%95%EF%BC%9ABeam-Search"><span class="post-toc-number">1.3.4.3.</span> <span class="post-toc-text">算法：Beam Search</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Planning-with-Beam-Search"><span class="post-toc-number">1.3.5.</span> <span class="post-toc-text">Planning with Beam Search</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#What-is-Beam-search"><span class="post-toc-number">1.3.5.1.</span> <span class="post-toc-text">What is Beam search</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Input-of-torch-transformer"><span class="post-toc-number">1.3.6.</span> <span class="post-toc-text">Input of torch.transformer</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">Decision Transformer: Reinforcement Learning via Sequence Modeling</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E4%BE%8B%E5%AD%90%EF%BC%9A%E9%80%89%E6%8B%A9%E4%B8%A4%E7%82%B9%E4%B9%8B%E9%97%B4%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84"><span class="post-toc-number">1.4.1.</span> <span class="post-toc-text">例子：选择两点之间最短路径</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Methods%E2%80%94%E2%80%94%E8%BD%A8%E8%BF%B9%E8%A1%A8%E7%A4%BA"><span class="post-toc-number">1.4.2.</span> <span class="post-toc-text">Methods——轨迹表示</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Methods%E2%80%94%E2%80%94Outline-of-the-algorithm"><span class="post-toc-number">1.4.3.</span> <span class="post-toc-text">Methods——Outline of the algorithm</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="post-toc-number">1.4.3.1.</span> <span class="post-toc-text">训练</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E6%8E%A8%E7%90%86"><span class="post-toc-number">1.4.3.2.</span> <span class="post-toc-text">推理</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Transformer-GPT"><span class="post-toc-number">1.4.4.</span> <span class="post-toc-text">Transformer-GPT</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#GPT%E5%8A%A8%E6%9C%BA"><span class="post-toc-number">1.4.4.1.</span> <span class="post-toc-text">GPT动机</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#PyTorch-nn-MultiheadAttention%E4%BD%BF%E7%94%A8"><span class="post-toc-number">1.4.4.2.</span> <span class="post-toc-text">PyTorch.nn.MultiheadAttention使用</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E8%BE%93%E5%85%A5"><span class="post-toc-number">1.4.4.3.</span> <span class="post-toc-text">输入</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Self-Mask%E6%9C%BA%E5%88%B6"><span class="post-toc-number">1.4.4.4.</span> <span class="post-toc-text">Self Mask机制</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Pytorch-nn-Transformer%E4%B8%AD%E7%9A%84Mask"><span class="post-toc-number">1.4.5.</span> <span class="post-toc-text">Pytorch.nn.Transformer中的Mask</span></a></li></ol></li></ol></li></ol>
    </ul>
    




<!-- Layouts -->

    <!-- Post Module -->
    <div class="material-post_container">

        <div class="material-post mdl-grid">
            <div class="mdl-card mdl-shadow--4dp mdl-cell mdl-cell--12-col">

                <!-- Post Header(Thumbnail & Title) -->
                
    <!-- Paradox Post Header -->
    
        
            <!-- Random Thumbnail -->
            <div class="post_thumbnail-random mdl-card__media mdl-color-text--grey-50">
            <script type="text/ls-javascript" id="post-thumbnail-script">
    var randomNum = Math.floor(Math.random() * 19 + 1);

    $('.post_thumbnail-random').attr('data-original', '/img/random/material-' + randomNum + '.png');
    $('.post_thumbnail-random').addClass('lazy');
</script>

        
    
            <p class="article-headline-p">
                RLTransformer
            </p>
        </div>





                
                    <!-- Paradox Post Info -->
                    <div class="mdl-color-text--grey-700 mdl-card__supporting-text meta">

    <!-- Author Avatar -->
    <div id="author-avatar">
        <img src="/img/avatar.png" width="44px" height="44px" alt="Author Avatar"/>
    </div>
    <!-- Author Name & Date -->
    <div>
        <strong>Gaoustcer</strong>
        <span>4月 04, 2023</span>
    </div>

    <div class="section-spacer"></div>

    <!-- Favorite -->
    <!--
        <button id="article-functions-like-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon btn-like">
            <i class="material-icons" role="presentation">favorite</i>
            <span class="visuallyhidden">favorites</span>
        </button>
    -->

    <!-- Qrcode -->
    

    <!-- Tags (bookmark) -->
    
    <button id="article-functions-viewtags-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
        <i class="material-icons" role="presentation">bookmark</i>
        <span class="visuallyhidden">bookmark</span>
    </button>
    <ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-functions-viewtags-button">
        <li class="mdl-menu__item">
        <a class="post_tag-none-link" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a>
    </ul>
    

    <!-- Share -->
    
        <button id="article-fuctions-share-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
    <i class="material-icons" role="presentation">share</i>
    <span class="visuallyhidden">share</span>
</button>
<ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-fuctions-share-button">
    

    

    <!-- Share Weibo -->
    
        <a class="post_share-link" href="http://service.weibo.com/share/share.php?appkey=&title=RLTransformer&url=http://gaoustcer.github.io/2023/04/04/RLTransformer/index.html&pic=http://gaoustcer.github.io/img/favicon.svg&searchPic=false&style=simple" target="_blank">
            <li class="mdl-menu__item">
                分享到微博
            </li>
        </a>
    

    <!-- Share Twitter -->
    
        <a class="post_share-link" href="https://twitter.com/intent/tweet?text=RLTransformer&url=http://gaoustcer.github.io/2023/04/04/RLTransformer/index.html&via=Gaoustcer" target="_blank">
            <li class="mdl-menu__item">
                分享到 Twitter
            </li>
        </a>
    

    <!-- Share Facebook -->
    
        <a class="post_share-link" href="https://www.facebook.com/sharer/sharer.php?u=http://gaoustcer.github.io/2023/04/04/RLTransformer/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 Facebook
            </li>
        </a>
    

    <!-- Share Google+ -->
    
        <a class="post_share-link" href="https://plus.google.com/share?url=http://gaoustcer.github.io/2023/04/04/RLTransformer/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 Google+
            </li>
        </a>
    

    <!-- Share LinkedIn -->
    

    <!-- Share QQ -->
    

    <!-- Share Telegram -->
    
</ul>

    
</div>

                

                <!-- Post Content -->
                <div id="post-content" class="mdl-color-text--grey-700 mdl-card__supporting-text fade out">
    
        <h1 id="RL-Transformer"><a href="#RL-Transformer" class="headerlink" title="RL Transformer"></a>RL Transformer</h1><p>回顾使用Sequential Net处理强化学习的一些工作</p>
<h2 id="回顾：Transformer"><a href="#回顾：Transformer" class="headerlink" title="回顾：Transformer"></a>回顾：Transformer</h2><p>只包括Attention和全连接</p>
<h3 id="回顾Attention机制"><a href="#回顾Attention机制" class="headerlink" title="回顾Attention机制"></a>回顾Attention机制</h3><p>输入状态(token)(decoder状态)</p>
<script type="math/tex; mode=display">
s_j</script><p>编码为query向量</p>
<script type="math/tex; mode=display">
q_j = W_q s_k</script><p>隐藏状态$h_1,\cdots,h_m$映射到key</p>
<script type="math/tex; mode=display">
k_i = W_k h_i\\
K = (k_i)</script><p>通过隐藏状态(Encoder)计算Value</p>
<script type="math/tex; mode=display">
v_i = W_v h_i</script><p>计算Attention系数</p>
<script type="math/tex; mode=display">
a_j = softmax(K^T q_j)\in \R^m</script><p>计算Attention下Value值</p>
<script type="math/tex; mode=display">
c_j=\sum_{i=1}^m \alpha_{ij}v_i</script><h3 id="Attention-for-Seq2Seq"><a href="#Attention-for-Seq2Seq" class="headerlink" title="Attention for Seq2Seq"></a>Attention for Seq2Seq</h3><p>Encoder inputs</p>
<script type="math/tex; mode=display">
x_1,x_2,\cdots,x_m</script><p>Decoder input</p>
<script type="math/tex; mode=display">
x_1^\prime,x_2^\prime,\cdots,x_t^\prime</script><p>Encoder编码输入为key和value</p>
<script type="math/tex; mode=display">
k_i = W_k x_i,K\in \R^{m\times key}\\
v_i =W_V x_i,V\in \R^{m\times  value}</script><p>Decoder输入同样进行线性变换，得到Query</p>
<script type="math/tex; mode=display">
q_j = W_Q x_j^\prime,Q\in \R^{t\times Query}</script><p>计算权重，取$q_1$和所有key组成的矩阵$K=(k_i)_{i=1}^m$，得到m维向量</p>
<script type="math/tex; mode=display">
\alpha_1 = K^T q_1\in\R^m</script><p>计算context vector $c_1$</p>
<script type="math/tex; mode=display">
c_1 = \alpha_{11} v_1 +\alpha_{21}v_2+\cdots + \alpha_{m1}v_m=V\alpha_1</script><p>写成</p>
<script type="math/tex; mode=display">
c_j = V\times softmax(K^T q_j)</script><p>对于Seq2Seq任务，得到$c_j$后经过Softmax Classifier得到概率分布，采样得到下一个特征编码作为$x_{t+1}^\prime$，这是Transformer在推理中的行为</p>
<h3 id="From-Attention-to-Self-Attention"><a href="#From-Attention-to-Self-Attention" class="headerlink" title="From Attention to Self-Attention"></a>From Attention to Self-Attention</h3><p>只包含一个输入序列$X=x_1,x_2,\cdots,x_n$，两个输入Sequence都是X</p>
<h3 id="From-Attention-to-Multi-head-Attention"><a href="#From-Attention-to-Multi-head-Attention" class="headerlink" title="From Attention to Multi-head Attention"></a>From Attention to Multi-head Attention</h3><p>多head attention有3L个参数矩阵，输入都是</p>
<script type="math/tex; mode=display">
x_1,x_2,\cdots,x_m</script><p>得到l个context sequence</p>
<script type="math/tex; mode=display">
c_{11},c_{12},\cdots,c_{1m}\\
c_{21},c_{22},\cdots,c_{2m}\\
\cdots\\
c_{L1},c_{L2},\cdots,c_{Lm}</script><p>将它们concat起来，得到</p>
<script type="math/tex; mode=display">
c_1 = (c_{11},c_{21},\cdots,c_{L1})</script><p>输入:$X\in \R^{m\times n}$,n是编码向量长度，输出$Y\in \R^{m\times n}$，使用skip connection将其连接</p>
<script type="math/tex; mode=display">
Y = X+Y</script><h3 id="From-Single-head-Self-Attention-to-Encoder"><a href="#From-Single-head-Self-Attention-to-Encoder" class="headerlink" title="From Single-head Self-Attention to Encoder"></a>From Single-head Self-Attention to Encoder</h3><p>输入</p>
<script type="math/tex; mode=display">
x_1,x_2,\cdots,x_m</script><p>经过Attention layer输出</p>
<script type="math/tex; mode=display">
c_1,c_2,\cdots,c_m</script><p>经过全连接(参数共享)变化为</p>
<script type="math/tex; mode=display">
u_1,u_2,\cdots,u_m</script><h3 id="Decoder网络"><a href="#Decoder网络" class="headerlink" title="Decoder网络"></a>Decoder网络</h3><p>假设Encoder输出Y满足</p>
<script type="math/tex; mode=display">
Y\in \R^{m\times n}</script><p>m是Encoder Sequence长度，将Decoder序列通过Multi-head attention得到</p>
<script type="math/tex; mode=display">
x_1^\prime \to c_1</script><p>再将两个Sequence</p>
<script type="math/tex; mode=display">
u_1,u_2,\cdots,u_m\\
c_1,c_2,\cdots,c_t</script><p>输入到一个新的Multi-head attention中，得到新的输出</p>
<script type="math/tex; mode=display">
z_1,z_2,\cdots,z_t\R^n</script><p>再通过全连接层，得到</p>
<script type="math/tex; mode=display">
z\to s</script><p>这样得到Decoder一个block</p>
<p><img src="https://s2.loli.net/2022/11/02/zuxZGrWJhn4dTMg.png" alt="image-20221102113758892"></p>
<p>$512\times m$来自Encoder输入</p>
<h2 id="Pytorch中Transformer的实现"><a href="#Pytorch中Transformer的实现" class="headerlink" title="Pytorch中Transformer的实现"></a>Pytorch中Transformer的实现</h2><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p><code>nn.Embedding</code>构造函数参数包括</p>
<ol>
<li>词汇表大小V</li>
<li>编码向量长度n</li>
</ol>
<p>这样对于输入的词$0,1,\cdots,V$，embedding层将其转化为对应的向量，embedding层实际上存储了int到长度为n的向量的映射</p>
<h3 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h3><p>对于输入序列$x_i$，增加位置编码</p>
<ol>
<li>$i=2k,PE=\sin(\frac{pos}{10000^{2i/d_{model}}})$</li>
<li>$i=2k+1,PE=\sin (\frac{pos}{10000^{2i/d_{modelvscod}}})$</li>
</ol>
<p>pos=i</p>
<p><img src="https://s2.loli.net/2022/11/03/Sh5OqnlrAxTULXW.png" alt="image-20221103155647449"></p>
<h3 id="Transformer-from-pytorch——target-Masking"><a href="#Transformer-from-pytorch——target-Masking" class="headerlink" title="Transformer from pytorch——target Masking"></a>Transformer from pytorch——target Masking</h3><p><em><em><em><em>，需要增加Mask去遮盖尚未出现的部分，encoder向量$X\in \R^{m\times n}$,n为输入词向量维度，m为输入sequence长度，经过encoder后编码为$\hat X\in \R^{m\times n}$大小的矩阵，对于输入decoder的target向量大小为$Y\in \R^{m_1\times n}$，</em></em>$，**不能将其完全暴露给输入encoding</em></em>给输入encoding**</p>
<h3 id="Padding-Masking"><a href="#Padding-Masking" class="headerlink" title="Padding Masking"></a>Padding Masking</h3><p>保证每个输入sequence长度相同，因此需要特殊的token</p>
<ol>
<li><SOS>表示一个sentence开始</li>
<li><EOS>表示一个sentence结束</li>
<li><PAD>用于补齐matrix的每一行</li>
</ol>
<p>为了告诉那些token仅仅用于补齐，我们需要为输入$X\in \R^{k\times m\times  n}$,k为输入句子数量，m为padding后的总长度，对应的padding matrix是一个binary矩阵，大小为$M\in \R^{k\times m}$</p>
<h2 id="Offline-Reinforcement-Learning-as-One-Big-Sequence-Modeling-Problem"><a href="#Offline-Reinforcement-Learning-as-One-Big-Sequence-Modeling-Problem" class="headerlink" title="Offline Reinforcement Learning as One Big Sequence Modeling Problem"></a>Offline Reinforcement Learning as One Big Sequence Modeling Problem</h2><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>将RL视为序列决策问题，Trajectory Transformer在动作-状态序列上序列，相比基于价值和策略的模型在长序列决策上更加有效</p>
<h3 id="轨迹"><a href="#轨迹" class="headerlink" title="轨迹"></a>轨迹</h3><p>表示为N-维状态，M维动作和标量奖励的序列</p>
<script type="math/tex; mode=display">
\tau= \{s_t^0,s_t^1,\cdots,s_t^{N-1},a_t^0,a_t^1,\cdots,a_t^{M-1},r_t \}_{t=0}^{T-1}</script><h3 id="构建字典"><a href="#构建字典" class="headerlink" title="构建字典"></a>构建字典</h3><p>连续状态动作需要进行离散化，假设</p>
<script type="math/tex; mode=display">
s_t^i \in [l^i,r^i )</script><p>离散化为</p>
<script type="math/tex; mode=display">
\overline s_t^i =[V\frac{S_t^i-l^i}{r^i -l^i}] + V_i</script><h3 id="原文（Reinforcement-Learning-and-Control-as-Sequence-Modeling）"><a href="#原文（Reinforcement-Learning-and-Control-as-Sequence-Modeling）" class="headerlink" title="原文（Reinforcement Learning and Control as Sequence Modeling）"></a>原文（Reinforcement Learning and Control as Sequence Modeling）</h3><p>对于N维状态空间和M维动作空间，建模为长度为T*(N+M+1)的序列，T为总的Transition数量</p>
<script type="math/tex; mode=display">
\tau = (s_t^1,s_t^2,\cdots,s_t^N,a_t^1,a_t^2,\cdots,a_t^M,r_t)</script><h4 id="连续空间离散化-用于输出似然概率"><a href="#连续空间离散化-用于输出似然概率" class="headerlink" title="连续空间离散化(用于输出似然概率)"></a>连续空间离散化(用于输出似然概率)</h4><ol>
<li>uniform，适用于任何维度的状态-动作都能被约束到有限范围内</li>
<li>分位数，事先需要统计先验概率分布</li>
</ol>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>最大化对数似然概率</p>
<p><img src="https://s2.loli.net/2022/11/01/nV9uWpJK6TlYmjU.png" alt="image-20221101114847359"></p>
<h4 id="算法：Beam-Search"><a href="#算法：Beam-Search" class="headerlink" title="算法：Beam Search"></a>算法：Beam Search</h4><p>输入：序列x，词汇表$\mathcal V$，序列长$T$，beam width $B$</p>
<h3 id="Planning-with-Beam-Search"><a href="#Planning-with-Beam-Search" class="headerlink" title="Planning with Beam Search"></a>Planning with Beam Search</h3><p>从三个角度看Sequence生成如何实现智能体控制</p>
<ol>
<li>模仿学习</li>
<li>基于目标导向的强化学习</li>
</ol>
<h4 id="What-is-Beam-search"><a href="#What-is-Beam-search" class="headerlink" title="What is Beam search"></a>What is Beam search</h4><p>对于Seq2Seq模型，Beam Search应用于推理时(训练时知道Ground Truth)，假设输入的Seq为</p>
<script type="math/tex; mode=display">
s_0s_1\cdots s_n</script><p>对于第一个输入token $s_0$,Transformer预测下一个token按照概率从大到小是</p>
<script type="math/tex; mode=display">
a_0,a_1,\cdots,a_m</script><p>选择概率最大的两个token，比如$a_0,a_1$，生成第二个token时需要将$a_0,a_1$和词汇表中所有词结合，生成如下序列</p>
<script type="math/tex; mode=display">
a_0b_0,a_0b_1,\cdots,a_0b_m,\cdots,a_1b_0,a_1b_1,\cdots,a_1b_m</script><p>在这些序列中选择似然最大的两个序列，进入第三轮，直到遇到结束符或者达到最大长度为止</p>
<blockquote>
<p>为什么不选择greedy search(我认为是增加强化学习中的搜索)</p>
</blockquote>
<p>定义序列单位似然</p>
<script type="math/tex; mode=display">
\log P_\theta(Y|x) =\sum_{y\in Y} \log P_\theta(y|x)</script><h3 id="Input-of-torch-transformer"><a href="#Input-of-torch-transformer" class="headerlink" title="Input of torch.transformer"></a>Input of <code>torch.transformer</code></h3><p>输入串</p>
<script type="math/tex; mode=display">
x_0x_1\cdots x_n</script><p>每个输入的token需要进行embedding，即构建映射</p>
<script type="math/tex; mode=display">
x_i\to s_i</script><p>embedding结果$s_i$包含token的位置信息，训练时我们有GroundTruth作为生成的翻译序列</p>
<script type="math/tex; mode=display">
y_0y_1\cdots y_m</script><p>给它增加一个起始符</p>
<blockquote>
<p>总的而言感到这篇文章不够简洁</p>
</blockquote>
<h2 id="Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling"><a href="#Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling" class="headerlink" title="Decision Transformer: Reinforcement Learning via Sequence Modeling"></a>Decision Transformer: Reinforcement Learning via Sequence Modeling</h2><h3 id="例子：选择两点之间最短路径"><a href="#例子：选择两点之间最短路径" class="headerlink" title="例子：选择两点之间最短路径"></a>例子：选择两点之间最短路径</h3><ol>
<li>设计reward：在目标节点上reward=0，否则为-1</li>
<li>在随机游走的数据上训练</li>
</ol>
<h3 id="Methods——轨迹表示"><a href="#Methods——轨迹表示" class="headerlink" title="Methods——轨迹表示"></a>Methods——轨迹表示</h3><p>轨迹表示为累积奖励-状态-动作对，注意当前状态根据未来累计奖励计算，因此对于一个长度为T的奖励序列</p>
<script type="math/tex; mode=display">
r_1,r_2,\cdots,r_T</script><p>定义t时刻之后的未来累计奖励$\hat {R_t}$为</p>
<script type="math/tex; mode=display">
\hat R_t = \sum_{t^\prime=t}^T r_{t^\prime}</script><p>轨迹记作</p>
<script type="math/tex; mode=display">
\tau = (\hat R_1 ,s_1,a_1,\hat R_2 ,s_2,a_2,\cdots,\hat R_T,s_T,a_T)</script><p>使用累计奖励：避免局部奖励陷入局部最优</p>
<h3 id="Methods——Outline-of-the-algorithm"><a href="#Methods——Outline-of-the-algorithm" class="headerlink" title="Methods——Outline of the algorithm"></a>Methods——Outline of the algorithm</h3><p>将3K个token(包含K个累计奖励、K个状态和K个动作)送入transformer，为序列中每种模态的数据使用线性层变换到embedding dimension</p>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>state,action and reward_to_go are mapped into same size m and add position embedding into them.</p>
<script type="math/tex; mode=display">
state\to eembedding\\
action\to aembedding\\
reward-to-go\to rembedding</script><p><em><em>从中取出对应a动作的部分</em></em><em><em>从中取出对应a动作的部分</em></em><em><em>从中取出对应a动作的部分</em></em>取出对应a动作的部分**</p>
<h4 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h4><p>推理和训练的区别在于：</p>
<ol>
<li>推理不知道动作</li>
<li>推理时不知道<code>reward-to-go</code></li>
</ol>
<p>因此需要设置一个expert-reward，根据每步奖励减去当前这步奖励r</p>
<h3 id="Transformer-GPT"><a href="#Transformer-GPT" class="headerlink" title="Transformer-GPT"></a>Transformer-GPT</h3><p>GPT保留Transformer中Decoder部分，每个模块由</p>
<ol>
<li>Mask Multi-head self-attention</li>
<li>feed forward</li>
</ol>
<p>构成，Mask Multi-head self-attention输入一个序列，最终得到和它长度相同的序列。</p>
<h4 id="GPT动机"><a href="#GPT动机" class="headerlink" title="GPT动机"></a>GPT动机</h4><p>基于无标注的数据进行预训练，在下游任务上进行少量finetune</p>
<h4 id="PyTorch-nn-MultiheadAttention使用"><a href="#PyTorch-nn-MultiheadAttention使用" class="headerlink" title="PyTorch.nn.MultiheadAttention使用"></a><code>PyTorch.nn.MultiheadAttention</code>使用</h4><p>初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=<span class="number">0.0</span>, bias=<span class="literal">True</span>, add_bias_kv=<span class="literal">False</span>, add_zero_attn=<span class="literal">False</span>, kdim=<span class="literal">None</span>, vdim=<span class="literal">None</span>, batch_first=<span class="literal">False</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>embed_dim，输入query,key,value维度</li>
<li>num_head 多头数量</li>
</ol>
<h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><p>输入Q,K,V张量，它们的维度可能不同，需要在<code>kdim</code>和<code>vdim</code>中设置，先通过线性变换转化为维度相同的矩阵，并编码为query,key,value向量</p>
<script type="math/tex; mode=display">
Q = QW_i^Q = (q_1,q_2,\cdots,q_n)\\
K = K W_i^K = (k_1,k_2,\cdots,k_m)\\
V = V W_i^V = (v_1,v_2,\cdots,v_m)\\
q_i,k_i,v_i\in\R^k</script><p>对于每个query，计算注意力系数</p>
<script type="math/tex; mode=display">
q_i \to K q_i= \lambda+_i\in \R^m</script><p>在此基础上$\lambda$和V加权计算$q_i$的Attention嵌入</p>
<script type="math/tex; mode=display">
a_i = V\lambda_i\in \R^k</script><p>n个multi-head针对一个query生成多个Attention vector $a_i^1,a_i^2,\cdots,a_i^n$，将其拼接</p>
<script type="math/tex; mode=display">
a_i = concat(a_i^1,a_i^2,\cdots,a_i^n)</script><p>通过全连接层得到最终的编码向量</p>
<h4 id="Self-Mask机制"><a href="#Self-Mask机制" class="headerlink" title="Self Mask机制"></a>Self Mask机制</h4><p>不希望在提前暴露未来时间的词向量</p>
<h3 id="Pytorch-nn-Transformer中的Mask"><a href="#Pytorch-nn-Transformer中的Mask" class="headerlink" title="Pytorch.nn.Transformer中的Mask"></a><code>Pytorch.nn.Transformer</code>中的Mask</h3><p>输入Mask的参数包括</p>
<ol>
<li>src 输入encoder的序列，假设为$(N,L,E)$，代表sequence数量，sequence padding长度，每个token维度</li>
<li>tgt 输入decoder的序列，假设为<code>(N,K,E)</code></li>
<li>src/tgt_mask</li>
<li>src/tgt_key_padding_mask</li>
</ol>
<p><em><em>只能看到上文信息</em></em><em><em>只能看到上文信息</em></em><em><em>只能看到上文信息</em></em><em><em>只能看到上文信息</em></em><em><em>只能看到上文信息</em></em><em><em>只能看到上文信息</em></em><em><em>只能看到上文信息</em></em><strong>只能看到上文信息</strong>，这里对src和tgt mask进行额外说明，假设src,tgt mask均为下三角为1的矩阵</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
1 & 0 & 0 & \cdots & 0 \\
1 & 1 & 0 & \cdots &0\\
1 & 1 & 1 & \cdots & 0\\
\vdots & \vdots & \vdots & \cdots  \\
1 & 1 & 1 & \cdots & 1
\end{pmatrix}</script><p>则对于输入sequence</p>
<script type="math/tex; mode=display">
x_1,x_2,\cdots,x_n</script><p>的embedding(对于src是encoder出的memory，对于tgt就是最后的输出)时，对于第i个token$x_i$，实际上是在一个子序列</p>
<script type="math/tex; mode=display">
x_1,x_2,\cdots,x_i</script><p>中计算Attention机制</p>
<p><code>_key_padding_mask</code>用于处理多个batch的序列长度不一样的情况，具体来说<code>src_key_padding_mask</code>大小为<code>(N,L)</code>，每一行是一个长度为L的binary向量，True表示对应的词向量实际上是补全句子长度所做的padding</p>

        
    

    
</div>


                

                <!-- Post Comments -->
                
                    
    <!-- 使用 DISQUS -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://gaoustcer.github.io/2023/04/04/RLTransformer/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://gaoustcer.github.io/2023/04/04/RLTransformer/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>
<script type="text/ls-javascript" id="disqus-thread-script">
    queue.offer(function() {
            (function() { // DON'T EDIT BELOW THIS LINE
                var d = document;
                var s = d.createElement('script');
                s.src = '//.disqus.com/embed.js';
                s.setAttribute('data-timestamp', + new Date());
                (d.head || d.body).appendChild(s);
            })();
        });
</script>

</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>

                
            </div>

            <!-- Post Prev & Next Nav -->
            <nav class="material-nav mdl-color-text--grey-50 mdl-cell mdl-cell--12-col">
    <!-- Prev Nav -->
    
        <a href="/2023/04/05/Convex-Optimization-set-and-functions/" id="post_nav-newer" class="prev-content">
            <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-color--white mdl-color-text--grey-900" role="presentation">
                <i class="material-icons">arrow_back</i>
            </button>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            新篇
        </a>
    

    <!-- Section Spacer -->
    <div class="section-spacer"></div>

    <!-- Next Nav -->
    
</nav>

        </div>
    </div>



                    
                        <!-- Overlay For Active Sidebar -->
<div class="sidebar-overlay"></div>

<!-- Material sidebar -->
<aside id="sidebar" class="sidebar sidebar-colored sidebar-fixed-left" role="navigation">
    <div id="sidebar-main">
        <!-- Sidebar Header -->
        <div class="sidebar-header header-cover" style="background-image: url(/img/sidebar_header.png);">
    <!-- Top bar -->
    <div class="top-bar"></div>

    <!-- Sidebar toggle button -->
    <button type="button" class="sidebar-toggle mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon" style="display: initial;" data-upgraded=",MaterialButton,MaterialRipple">
        <i class="material-icons">clear_all</i>
        <span class="mdl-button__ripple-container">
            <span class="mdl-ripple">
            </span>
        </span>
    </button>

    <!-- Sidebar Avatar -->
    <div class="sidebar-image">
        <img src="/img/avatar.png" alt="Gaoustcer's avatar">
    </div>

    <!-- Sidebar Email -->
    <a data-toggle="dropdown" class="sidebar-brand" href="#settings-dropdown">
        kd193104@mail.ustc.edu.cn
        <b class="caret"></b>
    </a>
</div>


        <!-- Sidebar Navigation  -->
        <ul class="nav sidebar-nav">
    <!-- User dropdown  -->
    <li class="dropdown">
        <ul id="settings-dropdown" class="dropdown-menu">
            
                <li>
                    <a href="#" target="_blank" title="Email Me">
                        
                            <i class="material-icons sidebar-material-icons sidebar-indent-left1pc-element">email</i>
                        
                        Email Me
                    </a>
                </li>
            
        </ul>
    </li>

    <!-- Homepage -->
    
        <li id="sidebar-first-li">
            <a href="/">
                
                    <i class="material-icons sidebar-material-icons">home</i>
                
                主页
            </a>
        </li>
        
    

    <!-- Archives  -->
    
        <li class="dropdown">
            <a href="#" class="ripple-effect dropdown-toggle" data-toggle="dropdown">
                
                    <i class="material-icons sidebar-material-icons">inbox</i>
                
                    归档
                <b class="caret"></b>
            </a>
            <ul class="dropdown-menu">
            <li>
                <a class="sidebar_archives-link" href="/archives/2023/10/">十月 2023<span class="sidebar_archives-count">7</span></a></li><li><a class="sidebar_archives-link" href="/archives/2023/09/">九月 2023<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2023/08/">八月 2023<span class="sidebar_archives-count">8</span></a></li><li><a class="sidebar_archives-link" href="/archives/2023/07/">七月 2023<span class="sidebar_archives-count">6</span></a></li><li><a class="sidebar_archives-link" href="/archives/2023/06/">六月 2023<span class="sidebar_archives-count">5</span></a></li><li><a class="sidebar_archives-link" href="/archives/2023/05/">五月 2023<span class="sidebar_archives-count">4</span></a></li><li><a class="sidebar_archives-link" href="/archives/2023/04/">四月 2023<span class="sidebar_archives-count">23</span></a>
            </ul>
        </li>
        
    

    <!-- Categories  -->
    

    <!-- Pages  -->
    
        <li>
            <a href="/about" title="About">
                
                    <i class="material-icons sidebar-material-icons">person</i>
                
                About
            </a>
        </li>
        
    
        <li>
            <a href="/papers" title="papers">
                
                    <i class="material-icons sidebar-material-icons">person</i>
                
                papers
            </a>
        </li>
        
    

    <!-- Article Number  -->
    
        <li>
            <a href="/archives">
                文章总数
                <span class="sidebar-badge">55</span>
            </a>
        </li>
        
    
</ul>


        <!-- Sidebar Footer -->
        <!--
I'm glad you use this theme, the development is no so easy, I hope you can keep the copyright, I will thank you so much.
If you still want to delete the copyrights, could you still retain the first one? Which namely "Theme Material"
It will not impact the appearance and can give developers a lot of support :)

很高兴您使用并喜欢该主题，开发不易 十分谢谢与希望您可以保留一下版权声明。
如果您仍然想删除的话 能否只保留第一项呢？即 "Theme Material"
它不会影响美观并可以给开发者很大的支持和动力。 :)
-->

<!-- Sidebar Divider -->

    <div class="sidebar-divider"></div>


<!-- Theme Material -->

    <a href="https://github.com/bollnh/hexo-theme-material"  class="sidebar-footer-text-a" target="_blank">
        <div class="sidebar-text mdl-button mdl-js-button mdl-js-ripple-effect sidebar-footer-text-div" data-upgraded=",MaterialButton,MaterialRipple">
            主题 - Material
            <span class="sidebar-badge badge-circle">i</span>
        </div>
    </a>


<!-- Help & Support -->
<!--

-->

<!-- Feedback -->
<!--

-->

<!-- About Theme -->
<!--

-->

    </div>

    <!-- Sidebar Image -->
    

</aside>

                    

                    
                        <!-- Footer Top Button -->
                        <div id="back-to-top" class="toTop-wrap">
    <a href="#top" class="toTop">
        <i class="material-icons footer_top-i">expand_less</i>
    </a>
</div>

                    

                    <!--Footer-->
<footer class="mdl-mini-footer" id="bottom">
    
        <!-- Paradox Footer Left Section -->
        <div class="mdl-mini-footer--left-section sns-list">
    <!-- Twitter -->
    
        <a href="https://twitter.com/twitter" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-twitter">
                <span class="visuallyhidden">Twitter</span>
            </button><!--
     --></a>
    

    <!-- Facebook -->
    
        <a href="https://www.facebook.com/facebook" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-facebook">
                <span class="visuallyhidden">Facebook</span>
            </button><!--
     --></a>
    

    <!-- Google + -->
    
        <a href="https://www.google.com/" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-gplus">
                <span class="visuallyhidden">Google Plus</span>
            </button><!--
     --></a>
    

    <!-- Weibo -->
    

    <!-- Instagram -->
    

    <!-- Tumblr -->
    

    <!-- Github -->
    

    <!-- LinkedIn -->
    

    <!-- Zhihu -->
    

    <!-- Bilibili -->
    

    <!-- Telegram -->
    

    <!-- V2EX -->
    

    <!-- Segmentfault -->
    
</div>


        <!--Copyright-->
        <div id="copyright">
            Copyright&nbsp;©&nbsp;<span year></span>&nbsp;Hexo
            
        </div>

        <!-- Paradox Footer Right Section -->

        <!--
        I am glad you use this theme, the development is no so easy, I hope you can keep the copyright.
        It will not impact the appearance and can give developers a lot of support :)

        很高兴您使用该主题，开发不易，希望您可以保留一下版权声明。
        它不会影响美观并可以给开发者很大的支持。 :)
        -->

        <div class="mdl-mini-footer--right-section">
            <div>
                <div class="footer-develop-div">Powered by <a href="https://hexo.io" target="_blank" class="footer-develop-a">Hexo</a></div>
                <div class="footer-develop-div">Theme - <a href="https://github.com/bollnh/hexo-theme-material" target="_blank" class="footer-develop-a">Material</a></div>
            </div>
        </div>
    
</footer>


                    <!-- Import JS File -->

    <script>lsloader.load("lazyload_js","/js/lazyload.min.js?1BcfzuNXqV+ntF6gq+5X3Q==", true)</script>



    <script>lsloader.load("js_js","/js/js.min.js?Bn9UzEm8RrBSxqyZB0zPjA==", true)</script>



    <script>lsloader.load("np_js","/js/nprogress.js?pl3Qhb9lvqR1FlyLUna1Yw==", true)</script>


<script type="text/ls-javascript" id="NProgress-script">
    NProgress.configure({
        showSpinner: true
    });
    NProgress.start();
    $('#nprogress .bar').css({
        'background': '#29d'
    });
    $('#nprogress .peg').css({
        'box-shadow': '0 0 10px #29d, 0 0 15px #29d'
    });
    $('#nprogress .spinner-icon').css({
        'border-top-color': '#29d',
        'border-left-color': '#29d'
    });
    setTimeout(function() {
        NProgress.done();
        $('.fade').removeClass('out');
    }, 800);
</script>









   <!-- 使用 DISQUS js 代码 -->
<script id="dsq-count-scr" src="//.disqus.com/count.js" async></script>





<!-- UC Browser Compatible -->
<!-- <script>
	var agent = navigator.userAgent.toLowerCase();
	if(agent.indexOf('ucbrowser')>0) {
		document.write('
<link rel="stylesheet" href="/css/uc.css">
');
	   alert('由于 UC 浏览器使用极旧的内核，而本网站使用了一些新的特性。\n为了您能更好的浏览，推荐使用 Chrome 或 Firefox 浏览器。');
	}
</script> -->

<!-- Import prettify js  -->



<!-- Window Load -->
<!-- add class for prettify -->
<script type="text/ls-javascript" id="window-load">
    $(window).on('load', function() {
        // Post_Toc parent position fixed
        $('.post-toc-wrap').parent('.mdl-menu__container').css('position', 'fixed');
    });

    
    
</script>

<!-- MathJax Load-->


<!-- Bing Background -->


<script type="text/ls-javascript" id="lazy-load">
    // Offer LazyLoad
    queue.offer(function(){
        $('.lazy').lazyload({
            effect : 'show'
        });
    });

    // Start Queue
    $(document).ready(function(){
        setInterval(function(){
            queue.execNext();
        },200);
    });
</script>

<!-- Custom Footer -->



<script>
    var copyrightNow = new Date().getFullYear();
    var textContent = document.querySelector('span[year]')

    copyrightSince = 0000;
    if (copyrightSince === copyrightNow||copyrightSince === 0000) {
        textContent.textContent = copyrightNow
    } else {
        textContent.textContent = copyrightSince + ' - ' + copyrightNow
    }

    (function(){
        var scriptList = document.querySelectorAll('script[type="text/ls-javascript"]')

        for (var i = 0; i < scriptList.length; ++i) {
            var item = scriptList[i];
            lsloader.runInlineScript(item.id,item.id);
        }
    })()
console.log('\n %c © Material Theme | Version: 1.5.6 | https://github.com/bollnh/hexo-theme-material %c \n', 'color:#455a64;background:#e0e0e0;padding:5px 0;border-top-left-radius:5px;border-bottom-left-radius:5px;', 'color:#455a64;background:#e0e0e0;padding:5px 0;border-top-right-radius:5px;border-bottom-right-radius:5px;');
</script>

                </main>
            </div>
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
    for (var i = 0; i < all.length; ++i)
        all[i].SourceElement().parentNode.className += ' has-jax';
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->

</body>
    
</html>
